{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import silhouette_score\n\n# Load and preprocess your data\ndata = pd.read_csv(\"your_data.csv\")\nX = data.drop(columns=[\"City\"])  # Assuming \"City\" is the column with city names\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Determine the optimal number of clusters using the Elbow Method\ninertia = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(X_scaled)\n    inertia.append(kmeans.inertia_)\n\nplt.plot(range(1, 11), inertia)\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Inertia\")\nplt.title(\"Elbow Method\")\nplt.show()\n\n# Based on the elbow method or other criteria, choose the number of clusters (K)\n\n# Apply K-Means clustering\nk = 3  # Adjust this based on your analysis\nkmeans = KMeans(n_clusters=k, random_state=42)\nclusters = kmeans.fit_predict(X_scaled)\n\n# Evaluate the quality of clusters using silhouette score\nsilhouette_avg = silhouette_score(X_scaled, clusters)\nprint(f\"Silhouette Score: {silhouette_avg}\")\n\n# Add cluster labels to the original data\ndata[\"Cluster\"] = clusters\n\n# Now, you have cities grouped into clusters. You can analyze the clusters and predict aid requirements for new cities based on cluster assignments.\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis')\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.title(\"Cluster Visualization (PCA)\")\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import silhouette_score\n\n# Load and preprocess your data\ndata = pd.read_csv(\"your_data.csv\")\nX = data.drop(columns=[\"City\"])  # Assuming \"City\" is the column with city names\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Determine the optimal number of clusters using the Elbow Method\ninertia = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(X_scaled)\n    inertia.append(kmeans.inertia_)\n\nplt.plot(range(1, 11), inertia)\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Inertia\")\nplt.title(\"Elbow Method\")\nplt.show()\n\n# Based on the elbow method or other criteria, choose the number of clusters (K)\n\n# Apply K-Means clustering\nk = 3  # Adjust this based on your analysis\nkmeans = KMeans(n_clusters=k, random_state=42)\nclusters = kmeans.fit_predict(X_scaled)\n\n# Visualize the clusters using PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis')\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.title(\"Cluster Visualization (PCA)\")\nplt.show()\n\n# Evaluate the quality of clusters using silhouette score\nsilhouette_avg = silhouette_score(X_scaled, clusters)\nprint(f\"Silhouette Score: {silhouette_avg}\")\n\n# Add cluster labels to the original data\ndata[\"Cluster\"] = clusters\n\n# Interpret the clusters and predict aid requirements for new cities\n# Implement real-time data handling, evaluation, and hyperparameter tuning as needed\n\n# Save the trained model for future use\nimport joblib\njoblib.dump(kmeans, \"kmeans_model.pkl\")\n\n# Load the model when needed\nloaded_kmeans = joblib.load(\"kmeans_model.pkl\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}